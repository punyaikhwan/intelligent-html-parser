# FLAN-T5 HTML Parser Training Configuration

# Model Configuration
model_name: "google/flan-t5-small"
output_dir: "./output/flan-t5-parser-enhanced-lr-3e4-5-epochs-wup30"

# Data Files
full_train_file: "training_data.json"  # Full dataset for k-fold. For non-kfold, will be split into train/val if train_file/validation_file are null
train_file: null  # Overridden if use_kfold is true
validation_file: null  # Overridden if use_kfold is true

# Training Parameters
num_epochs: 5
batch_size: 4  # Reduce batch size significantly for memory constraints
learning_rate: 3e-4
warmup_steps: 30  # Reduce warmup steps
weight_decay: 0.01
max_input_length: 1024  # Reduce input length to save memory
max_output_length: 256  # Reduce output length to save memory

# Evaluation Parameters - Parameter Evaluasi
use_kfold: false  # Enable k-fold for better validation 
kfold_splits: 5
validation_split: 0.2
eval_steps: 100  # Reduce evaluation frequency to save time
save_steps: 200  # Save less frequently
save_total_limit: 2  # Keep fewer checkpoints to save disk space
metric_for_best_model: "rougeL"

# Data Parameters - Parameter Data  
shuffle_data: true

# Hardware Configuration - Konfigurasi Hardware (Memory Optimized)
use_fp16: true  # Enable fp16 to reduce memory usage by ~50%
gradient_accumulation_steps: 8  # Increase to maintain effective batch size of 32
logging_steps: 50  # Log moderately
dataloader_pin_memory: false  # Disable pin memory to save RAM